# Olympics and Countries Data Processing Pipeline

## Overview
This project is a data processing pipeline that combines multiple datasets related to the Olympics and countries. I have implemented this assessment in two ways:
1. Using a Jupyter notebook with Pandas for exploratory data analysis and processing.
2. Using PySpark to handle larger datasets and demonstrate distributed processing capabilities.

## Project Structure
- `datasets/`: Contains the CSV and JSON data files used in the pipeline.
- `notebooks/`: Contains the Jupyter notebook (`holman_de_test.ipynb`) with Pandas implementation.
- `spark_implementation/`: Contains the PySpark script (`etl_jobs.py`).
- `output_files/`: Contains the output CSV and Parquet files generated by the pipeline.
- `README.md`: This file with instructions for running the project.
- `requirements.txt`: List of Python dependencies needed to run the project.

## Requirements
- Python 3.8 or above
- PySpark
- Pandas
- Jupyter Notebook
- A Spark cluster or local Spark setup (for running the Spark script)

## Installation
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <repository-directory>
2.
```bash
pip install -r requirements.txt
```
3.Run jupyter notebook under 'notebooks/holman_de_test.ipynb'
4. For spark cd into spark_implementation/etl folder and enter 
```bash
spark-submit etl_jobs.py
```

Note on Spark Performance:
Running the pipeline with PySpark is more efficient for large datasets because Spark distributes the processing across multiple nodes (in a cluster setup) or cores (in a local setup). This can significantly reduce execution time compared to running the same pipeline with Pandas, especially when dealing with gigabytes or terabytes of data. In a local setup, the performance gain might be less noticeable, but the ability to scale seamlessly is a major advantage when using Spark.
